name: Docker Ephemeral Tests

on: [pull_request]

# Add explicit permissions for commenting on PRs
permissions:
  contents: read
  pull-requests: write
  actions: read

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Build unit test image
        run: docker build -f dockerfile.unit_test -t unit-test .

      - name: Run unit tests
        run: |
          mkdir -p test-results/unit
          docker run \
            --rm \
            -v $(pwd)/test-results/unit:/app/test-results \
            unit-test \
            pytest --junitxml=/app/test-results/results.xml --tb=short tests/unit/

      - name: Publish unit test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results
          path: test-results/unit/results.xml

#  integration-tests:
#    needs: unit-tests
#    runs-on: ubuntu-latest
#    services:
#      postgres:
#        image: postgres:14
#        env:
#          POSTGRES_PASSWORD: test
#          POSTGRES_USER: test
#          POSTGRES_DB: test
#        ports:
#          - 5432:5432
#        options: >-
#          --health-cmd pg_isready
#          --health-interval 10s
#          --health-timeout 5s
#          --health-retries 5
#
#    steps:
#      - uses: actions/checkout@v4
#
#      - name: Run integration tests
#        run: |
#          mkdir -p test-results/integration
#          docker-compose -f docker-compose.integration.yml up \
#            --build \
#            --abort-on-container-exit \
#            --exit-code-from app
#
#      - name: Save integration test results
#        if: always()
#        run: |
#          docker cp $(docker ps -aqf "name=app"):/app/test-results/integration/results.xml test-results/integration/
#
#      - name: Publish integration test results
#        uses: actions/upload-artifact@v4  # Updated from v3 → v4
#        if: always()
#        with:
#          name: integration-test-results
#          path: test-results/integration/results.xml

  report:
    needs: [unit-tests]
    runs-on: ubuntu-latest
    if: always()  # Always run to ensure comment is posted even if tests fail
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: unit-test-results
          path: test-results/unit

      - name: Generate Test Summary Report
        run: |
          echo "## 🧪 Unit Test Results" > test-summary.md
          echo "" >> test-summary.md
          
          # Debug: Show XML file content
          echo "=== Debug: XML File Content ===" >&2
          if [ -f "test-results/unit/results.xml" ]; then
            cat test-results/unit/results.xml >&2
          else
            echo "XML file not found!" >&2
          fi
          echo "=== End Debug ===" >&2
          
          if [ -f "test-results/unit/results.xml" ]; then
            # Parse XML results and generate summary
            echo "### 📊 Test Summary" >> test-summary.md
            echo "" >> test-summary.md
            
            # Extract test counts from XML - use more reliable parsing
            total_tests=$(grep -c '<testcase' test-results/unit/results.xml 2>/dev/null || echo "0")
            failed_tests=$(grep -c '<failure' test-results/unit/results.xml 2>/dev/null || echo "0")
            error_tests=$(grep -c '<error' test-results/unit/results.xml 2>/dev/null || echo "0")
            skipped_tests=$(grep -c '<skipped' test-results/unit/results.xml 2>/dev/null || echo "0")
            
            # Ensure all variables are numbers
            total_tests=${total_tests:-0}
            failed_tests=${failed_tests:-0}
            error_tests=${error_tests:-0}
            skipped_tests=${skipped_tests:-0}
            
            # Debug: Show parsed counts
            echo "Debug: total=$total_tests, failed=$failed_tests, error=$error_tests, skipped=$skipped_tests" >&2
            
            # Calculate passed tests (total - failed - errors - skipped)
            passed_tests=$((total_tests - failed_tests - error_tests - skipped_tests))
            
            # Ensure we don't have negative numbers
            if [ "$passed_tests" -lt 0 ]; then
              passed_tests=0
            fi
            
            # Create visual status indicators
            if [ "$failed_tests" -eq 0 ] && [ "$error_tests" -eq 0 ]; then
              echo "✅ **All tests passed!** ($total_tests/$total_tests)" >> test-summary.md
              echo "" >> test-summary.md
              echo "🎉 Great job! All unit tests are passing." >> test-summary.md
            else
              echo "❌ **Tests failed!** ($passed_tests/$total_tests passed, $((failed_tests + error_tests)) failed)" >> test-summary.md
              echo "" >> test-summary.md
              echo "⚠️  Some tests are failing. Please fix them before merging." >> test-summary.md
            fi
            
            echo "" >> test-summary.md
            echo "### 📈 Test Statistics" >> test-summary.md
            echo "- **Total Tests:** $total_tests" >> test-summary.md
            echo "- **Passed:** $passed_tests" >> test-summary.md
            echo "- **Failed:** $failed_tests" >> test-summary.md
            if [ "$error_tests" -gt 0 ]; then
              echo "- **Errors:** $error_tests" >> test-summary.md
            fi
            if [ "$skipped_tests" -gt 0 ]; then
              echo "- **Skipped:** $skipped_tests" >> test-summary.md
            fi
            
            # Calculate success rate safely
            if [ "$total_tests" -gt 0 ] && [ "$total_tests" -ne 0 ]; then
              success_rate=$(( (passed_tests * 100) / total_tests ))
              echo "- **Success Rate:** $success_rate%" >> test-summary.md
            else
              echo "- **Success Rate:** 0%" >> test-summary.md
            fi
            
            # Add test details if there are failures or errors
            if [ "$failed_tests" -gt 0 ] || [ "$error_tests" -gt 0 ]; then
              echo "" >> test-summary.md
              echo "### 🔍 Failed Tests Details" >> test-summary.md
              
              # Extract and format failure details - simplified approach
              if [ "$failed_tests" -gt 0 ]; then
                echo "#### ❌ Test Failures:" >> test-summary.md
                # Use a simpler approach to extract test names
                grep -A 5 '<testcase' test-results/unit/results.xml 2>/dev/null | while IFS= read -r line; do
                  if [[ $line == *"name="* ]]; then
                    test_name=$(echo "$line" | sed -n 's/.*name="\([^"]*\)".*/\1/p')
                    if [ ! -z "$test_name" ]; then
                      echo "- **$test_name**" >> test-summary.md
                    fi
                  fi
                done
              fi
              
              echo "" >> test-summary.md
              echo "### 📋 Raw Test Results" >> test-summary.md
              echo "```xml" >> test-summary.md
              cat test-results/unit/results.xml >> test-summary.md
              echo "```" >> test-summary.md
            fi
            
          else
            echo "❌ **No test results found**" >> test-summary.md
            echo "" >> test-summary.md
            echo "Tests may have failed before generating results. Check the workflow logs for details." >> test-summary.md
          fi

      - name: Debug - Show generated report
        run: |
          echo "=== Generated Test Summary ==="
          cat test-summary.md
          echo "=== End of Summary ==="

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const testSummary = fs.readFileSync('test-summary.md', 'utf8');
            
            console.log('Attempting to comment on PR...');
            console.log('PR Number:', context.issue.number);
            console.log('Repository:', context.repo.owner + '/' + context.repo.repo);
            
            // Create or update PR comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });
            
            console.log('Found', comments.length, 'existing comments');
            
            // Find existing test result comment
            const existingComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('🧪 Unit Test Results')
            );
            
            if (existingComment) {
              console.log('Updating existing comment:', existingComment.id);
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: testSummary
              });
            } else {
              console.log('Creating new comment');
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: testSummary
              });
            }
            
            console.log('Comment operation completed successfully');

      - name: Upload Test Summary
        uses: actions/upload-artifact@v4
        with:
          name: test-summary
          path: test-summary.md