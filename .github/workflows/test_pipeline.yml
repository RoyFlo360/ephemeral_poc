name: Docker Ephemeral Tests

on: [pull_request]

# Add explicit permissions for commenting on PR
permissions:
  contents: read
  pull-requests: write
  actions: read

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Build unit test image
        run: docker build -f dockerfile.unit_test -t unit-test .

      - name: Run unit tests
        run: |
          mkdir -p test-results/unit
          docker run \
            --rm \
            -v $(pwd)/test-results/unit:/app/test-results \
            unit-test \
            pytest --junitxml=/app/test-results/results.xml --tb=short tests/unit/

      - name: Publish unit test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results
          path: test-results/unit/results.xml

  integration-tests:
    needs: unit-tests
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: test
          POSTGRES_USER: test
          POSTGRES_DB: test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      - name: Install Docker Compose (if needed)
        run: |
          echo "=== Docker Environment Check ==="
          echo "Docker version:"
          docker --version
          echo ""
          echo "Docker Compose version:"
          docker compose version 2>/dev/null || echo "docker compose not available"
          echo ""
          echo "Checking for docker-compose:"
          which docker-compose 2>/dev/null || echo "docker-compose not found in PATH"
          echo "=== End Docker Check ==="
          
          # Check if docker compose is available
          if ! docker compose version >/dev/null 2>&1; then
            echo "Installing Docker Compose..."
            # Install docker-compose as fallback
            sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
            sudo chmod +x /usr/local/bin/docker-compose
            echo "Docker Compose installed as fallback"
          else
            echo "Docker Compose is available"
          fi

      - name: Run integration tests
        run: |
          mkdir -p test-results/integration
          # Use docker compose if available, otherwise fall back to docker-compose
          if docker compose version >/dev/null 2>&1; then
            echo "Using 'docker compose'"
            docker compose -f docker-compose.integration.yml up \
              --build \
              --abort-on-container-exit \
              --exit-code-from app \
              --remove-orphans
          else
            echo "Using 'docker-compose' fallback"
            docker-compose -f docker-compose.integration.yml up \
              --build \
              --abort-on-container-exit \
              --exit-code-from app \
              --remove-orphans
          fi
      
      - name: Check docker-compose logs
        if: always()
        run: |
          echo "=== Docker Compose Logs ==="
          # Use docker compose if available, otherwise fall back to docker-compose
          if docker compose version >/dev/null 2>&1; then
            docker compose -f docker-compose.integration.yml logs app || echo "Failed to get app logs"
          else
            docker-compose -f docker-compose.integration.yml logs app || echo "Failed to get app logs"
          fi
          echo "=== End Logs ==="
      
      - name: Check test results directory
        if: always()
        run: |
          echo "=== Test Results Directory Check ==="
          echo "Current directory: $(pwd)"
          echo "test-results directory exists: $(test -d test-results && echo "Yes" || echo "No")"
          if [ -d test-results ]; then
            echo "test-results contents:"
            ls -la test-results/
            if [ -d test-results/integration ]; then
              echo "test-results/integration contents:"
              ls -la test-results/integration/
            fi
          fi
          echo "=== End Directory Check ==="

      - name: Save integration test results
        if: always()
        run: |
          echo "=== Debug: Container Discovery ==="
          echo "Current directory: $(pwd)"
          echo "Directory contents:"
          ls -la
          
          echo ""
          echo "All running containers:"
          docker ps -a
          
          echo ""
          echo "All containers (including stopped):"
          docker ps -aq
          
          echo ""
          echo "=== End Debug ==="
          
          # Try to find the app container by specific name first
          CONTAINER_ID=$(docker ps -aqf "name=integration-test-app" 2>/dev/null || echo "")
          echo "Container ID for 'integration-test-app': $CONTAINER_ID"
          
          # If not found, try alternative naming patterns
          if [ -z "$CONTAINER_ID" ]; then
            CONTAINER_ID=$(docker ps -aqf "name=ephemeral_poc-app-1" 2>/dev/null || echo "")
            echo "Container ID for 'ephemeral_poc-app-1': $CONTAINER_ID"
          fi
          
          # If still not found, try to find any container with test results
          if [ -z "$CONTAINER_ID" ]; then
            CONTAINER_ID=$(docker ps -aqf "ancestor=ephemeral_poc_app" 2>/dev/null || echo "")
            echo "Container ID for 'ancestor=ephemeral_poc_app': $CONTAINER_ID"
          fi
          
          # If container found, copy results
          if [ ! -z "$CONTAINER_ID" ]; then
            echo "Found container: $CONTAINER_ID"
            docker cp "$CONTAINER_ID:/app/test-results/integration/results.xml" test-results/integration/ || echo "Failed to copy from container"
          else
            echo "No container found, checking if results exist in mounted volume"
          fi
          
          # Also check if results were written to the mounted volume
          if [ -f "test-results/integration/results.xml" ]; then
            echo "Test results found in mounted volume"
          else
            echo "No test results found, creating empty results file"
            mkdir -p test-results/integration
            echo '<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="integration" tests="0" failures="0" errors="0" skipped="0" time="0.0"></testsuite></testsuites>' > test-results/integration/results.xml
          fi

      - name: Publish integration test results
        uses: actions/upload-artifact@v4  # Updated from v3 → v4
        if: always()
        with:
          name: integration-test-results
          path: test-results/integration/results.xml

  selenium-tests:
    needs: [unit-tests, integration-tests]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Docker Compose (if needed)
        run: |
          echo "=== Docker Environment Check ==="
          echo "Docker version:"
          docker --version
          echo ""
          echo "Docker Compose version:"
          docker compose version 2>/dev/null || echo "docker compose not available"
          echo ""
          echo "Checking for docker-compose:"
          which docker-compose 2>/dev/null || echo "docker-compose not found in PATH"
          echo "=== End Docker Check ==="
          
          # Check if docker compose is available
          if ! docker compose version >/dev/null 2>&1; then
            echo "Installing Docker Compose..."
            # Install docker-compose as fallback
            sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
            sudo chmod +x /usr/local/bin/docker-compose
            echo "Docker Compose installed as fallback"
          else
            echo "Docker Compose is available"
          fi

      - name: Run selenium tests
        run: |
          mkdir -p test-results/selenium
          # Use docker compose if available, otherwise fall back to docker-compose
          if docker compose version >/dev/null 2>&1; then
            echo "Using 'docker compose'"
            docker compose -f docker-compose.selenium.yml up \
              --build \
              --abort-on-container-exit \
              --exit-code-from selenium-tests \
              --remove-orphans
          else
            echo "Using 'docker-compose' fallback"
            docker-compose -f docker-compose.selenium.yml up \
              --build \
              --abort-on-container-exit \
              --exit-code-from selenium-tests \
              --remove-orphans
          fi
      
      - name: Check docker-compose logs
        if: always()
        run: |
          echo "=== Docker Compose Logs ==="
          # Use docker compose if available, otherwise fall back to docker-compose
          if docker compose version >/dev/null 2>&1; then
            docker compose -f docker-compose.selenium.yml logs selenium-tests || echo "Failed to get selenium-tests logs"
          else
            docker-compose -f docker-compose.selenium.yml logs selenium-tests || echo "Failed to get selenium-tests logs"
          fi
          echo "=== End Logs ==="
      
      - name: Check test results directory
        if: always()
        run: |
          echo "=== Test Results Directory Check ==="
          echo "Current directory: $(pwd)"
          echo "test-results directory exists: $(test -d test-results && echo "Yes" || echo "No")"
          if [ -d test-results ]; then
            echo "test-results contents:"
            ls -la test-results/
            if [ -d test-results/selenium ]; then
              echo "test-results/selenium contents:"
              ls -la test-results/selenium/
            fi
          fi
          echo "=== End Directory Check ==="

      - name: Save selenium test results
        if: always()
        run: |
          echo "=== Debug: Container Discovery ==="
          echo "Current directory: $(pwd)"
          echo "Directory contents:"
          ls -la
          
          echo ""
          echo "All running containers:"
          docker ps -a
          
          echo ""
          echo "All containers (including stopped):"
          docker ps -aq
          
          echo ""
          echo "=== End Debug ==="
          
          # Try to find the selenium-tests container by specific name first
          CONTAINER_ID=$(docker ps -aqf "name=selenium-test-container" 2>/dev/null || echo "")
          echo "Container ID for 'selenium-test-container': $CONTAINER_ID"
          
          # If not found, try alternative naming patterns
          if [ -z "$CONTAINER_ID" ]; then
            CONTAINER_ID=$(docker ps -aqf "name=ephemeral_poc-selenium-tests-1" 2>/dev/null || echo "")
            echo "Container ID for 'ephemeral_poc-selenium-tests-1': $CONTAINER_ID"
          fi
          
          # If still not found, try to find any container with test results
          if [ -z "$CONTAINER_ID" ]; then
            CONTAINER_ID=$(docker ps -aqf "ancestor=ephemeral_poc_selenium-tests" 2>/dev/null || echo "")
            echo "Container ID for 'ancestor=ephemeral_poc_selenium-tests': $CONTAINER_ID"
          fi
          
          # If container found, copy results
          if [ ! -z "$CONTAINER_ID" ]; then
            echo "Found container: $CONTAINER_ID"
            docker cp "$CONTAINER_ID:/app/test-results/selenium/results.xml" test-results/selenium/ || echo "Failed to copy from container"
          else
            echo "No container found, checking if results exist in mounted volume"
          fi
          
          # Also check if results were written to the mounted volume
          if [ -f "test-results/selenium/results.xml" ]; then
            echo "Test results found in mounted volume"
          else
            echo "No test results found, creating empty results file"
            mkdir -p test-results/selenium
            echo '<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="selenium" tests="0" failures="0" errors="0" skipped="0" time="0.0"></testsuite></testsuites>' > test-results/selenium/results.xml
          fi

      - name: Publish selenium test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: selenium-test-results
          path: test-results/selenium/results.xml

  report:
    needs: [unit-tests, integration-tests, selenium-tests]
    runs-on: ubuntu-latest
    if: always()  # Always run to ensure comment is posted even if tests fail
    steps:
      - name: Download unit test results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: unit-test-results
          path: test-results/unit
      
      - name: Download integration test results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: integration-test-results
          path: test-results/integration
      
      - name: Download selenium test results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: selenium-test-results
          path: test-results/selenium

      - name: Generate Test Summary Report
        run: |
          # Initialize unit, integration & selenium test variables
          unit_suite_total=0
          unit_suite_failures=0
          unit_suite_errors=0
          unit_suite_skipped=0

          int_total_tests=0
          int_passed_tests=0
          int_failed_tests=0
          int_error_tests=0
          int_skipped_tests=0
          
          sel_total_tests=0
          sel_passed_tests=0
          sel_failed_tests=0
          sel_error_tests=0
          sel_skipped_tests=0
          
          echo "## 🧪 Unit Test Results" > test-summary.md
          echo "" >> test-summary.md
          
          # Debug: Show XML file content
          echo "=== Debug: XML File Content ===" >&2
          if [ -f "test-results/unit/results.xml" ]; then
            cat test-results/unit/results.xml >&2
          else
            echo "Unit test XML file not found!" >&2
          fi
          echo "=== End Debug ===" >&2
          
          # Check if unit test results exist
          if [ -f "test-results/unit/results.xml" ]; then
            # Parse XML results and generate summary
            echo "### 📊 Test Summary" >> test-summary.md
            echo "" >> test-summary.md
            
            # Extract test counts from XML - use more reliable parsing
            # First try to get counts from testsuite attributes (most reliable)
            unit_suite_total=$(grep -o 'tests="[0-9]*"' test-results/unit/results.xml 2>/dev/null | sed 's/tests="\([0-9]*\)"/\1/' || echo "0")
            unit_suite_failures=$(grep -o 'failures="[0-9]*"' test-results/unit/results.xml 2>/dev/null | sed 's/failures="\([0-9]*\)"/\1/' || echo "0")
            unit_suite_errors=$(grep -o 'errors="[0-9]*"' test-results/unit/results.xml 2>/dev/null | sed 's/errors="\([0-9]*\)"/\1/' || echo "0")
            unit_suite_skipped=$(grep -o 'skipped="[0-9]*"' test-results/unit/results.xml 2>/dev/null | sed 's/skipped="\([0-9]*\)"/\1/' || echo "0")
            
            # Fallback: Count individual elements
            unit_element_total=$(grep -c '<testcase' test-results/unit/results.xml 2>/dev/null || echo "0")
            unit_element_failures=$(grep -c '<failure' test-results/unit/results.xml 2>/dev/null || echo "0")
            unit_element_errors=$(grep -c '<error' test-results/unit/results.xml 2>/dev/null || echo "0")
            unit_element_skipped=$(grep -c '<skipped' test-results/unit/results.xml 2>/dev/null || echo "0")
            
            # Use testsuite attributes if available, otherwise fall back to element counts
            if [ "$unit_suite_total" != "0" ] && [ "$unit_suite_total" != "" ]; then
              total_tests=$unit_suite_total
              failed_tests=$unit_suite_failures
              error_tests=$unit_suite_errors
              skipped_tests=$unit_suite_skipped
              echo "Using testsuite attributes" >&2
            else
              total_tests=$unit_element_total
              failed_tests=$unit_element_failures
              error_tests=$unit_element_errors
              skipped_tests=$element_skipped
              echo "Using element counts" >&2
            fi
            
            # Clean up any whitespace and ensure they are valid numbers
            total_tests=$(echo "$total_tests" | tr -d '[:space:]')
            failed_tests=$(echo "$failed_tests" | tr -d '[:space:]')
            error_tests=$(echo "$error_tests" | tr -d '[:space:]')
            skipped_tests=$(echo "$skipped_tests" | tr -d '[:space:]')
            
            # Set default values if empty
            total_tests=${total_tests:-0}
            failed_tests=${failed_tests:-0}
            error_tests=${error_tests:-0}
            skipped_tests=${skipped_tests:-0}
            
            # Debug: Show final parsed counts
            echo "Final counts: total='$total_tests', failed='$failed_tests', error='$error_tests', skipped='$skipped_tests'" >&2
            echo "=== End XML Parsing Debug ===" >&2
            
            # Test arithmetic operations
            echo "Testing arithmetic: total=$total_tests, failed=$failed_tests" >&2
            echo "Arithmetic test: $((total_tests + 0))" >&2
            
            # Calculate passed tests (total - failed - errors - skipped)
            passed_tests=$((total_tests - failed_tests - error_tests - skipped_tests))
            
            # Ensure we don't have negative numbers
            if [ "$passed_tests" -lt 0 ]; then
              passed_tests=0
            fi
            
            # Create visual status indicators
            if [ "$failed_tests" -eq 0 ] && [ "$error_tests" -eq 0 ]; then
              echo "✅ **All tests passed!** ($total_tests/$total_tests)" >> test-summary.md
              echo "" >> test-summary.md
              echo "🎉 Great job! All unit tests are passing." >> test-summary.md
            else
              echo "❌ **Tests failed!** ($passed_tests/$total_tests passed, $((failed_tests + error_tests)) failed)" >> test-summary.md
              echo "" >> test-summary.md
              echo "⚠️  Some tests are failing. Please fix them before merging." >> test-summary.md
            fi
            
            echo "" >> test-summary.md
            echo "### 📈 Test Statistics" >> test-summary.md
            echo "- **Total Tests:** $total_tests" >> test-summary.md
            echo "- **Passed:** $passed_tests" >> test-summary.md
            echo "- **Failed:** $failed_tests" >> test-summary.md
            if [ "$error_tests" -gt 0 ]; then
              echo "- **Errors:** $error_tests" >> test-summary.md
            fi
            if [ "$skipped_tests" -gt 0 ]; then
              echo "- **Skipped:** $skipped_tests" >> test-summary.md
            fi
            
            # Calculate success rate safely
            if [ "$total_tests" -gt 0 ]; then
              success_rate=$(( (passed_tests * 100) / total_tests ))
              echo "- **Success Rate:** $success_rate%" >> test-summary.md
            else
              echo "- **Success Rate:** 0%" >> test-summary.md
            fi
            
            # Add test details if there are failures or errors
            if [ "$failed_tests" -gt 0 ] || [ "$error_tests" -gt 0 ]; then
              echo "" >> test-summary.md
              echo "### 🔍 Failed Tests Details" >> test-summary.md
              
              # Extract and format failure details - improved approach
              if [ "$failed_tests" -gt 0 ]; then
                echo "#### ❌ Test Failures:" >> test-summary.md
                # Extract test names and failure messages
                grep -A 10 '<testcase' test-results/unit/results.xml 2>/dev/null | while IFS= read -r line; do
                  if [[ $line == *"name="* ]]; then
                    test_name=$(echo "$line" | sed -n 's/.*name="\([^"]*\)".*/\1/p')
                    if [ ! -z "$test_name" ]; then
                      echo "- **$test_name**" >> test-summary.md
                    fi
                  elif [[ $line == *"<failure"* ]]; then
                    # Extract failure message
                    failure_msg=$(echo "$line" | sed -n 's/.*<failure[^>]*>\(.*\)<\/failure>.*/\1/p')
                    if [ ! -z "$failure_msg" ]; then
                      echo "  - Failure: \`$failure_msg\`" >> test-summary.md
                    fi
                  fi
                done
              fi
              
              echo "" >> test-summary.md
              echo "### 📋 Raw Test Results" >> test-summary.md
              echo '```xml' >> test-summary.md
              
              # Pretty print the XML for better readability
              if command -v xmllint >/dev/null 2>&1; then
                # Use xmllint if available (most Linux systems)
                xmllint --format test-results/unit/results.xml >> test-summary.md
              else
                # Use Python for pretty printing if xmllint not available
                python3 -c "import xml.dom.minidom; import sys; dom = xml.dom.minidom.parse('test-results/unit/results.xml'); print(dom.toprettyxml(indent='  '))" >> test-summary.md 2>/dev/null || cat test-results/unit/results.xml >> test-summary.md
              fi
              
              echo '```' >> test-summary.md
            fi
            
          else
            echo "❌ **No unit test results found**" >> test-summary.md
            echo "" >> test-summary.md
            echo "Unit tests may have failed before generating results. Check the workflow logs for details." >> test-summary.md
          fi
          
          # Integration Test Results Section
          echo "" >> test-summary.md
          echo "---" >> test-summary.md
          echo "" >> test-summary.md
          echo "## 🗄️ Integration Test Results" >> test-summary.md
          echo "" >> test-summary.md
          
          # Check if integration test results exist
          if [ -f "test-results/integration/results.xml" ]; then
            # Parse integration test XML results
            echo "### 📊 Integration Test Summary" >> test-summary.md
            echo "" >> test-summary.md
            
            # Extract test counts from integration test XML
            int_unit_suite_total=$(grep -o 'tests="[0-9]*"' test-results/integration/results.xml 2>/dev/null | sed 's/tests="\([0-9]*\)"/\1/' || echo "0")
            int_unit_suite_failures=$(grep -o 'failures="[0-9]*"' test-results/integration/results.xml 2>/dev/null | sed 's/failures="\([0-9]*\)"/\1/' || echo "0")
            int_unit_suite_errors=$(grep -o 'errors="[0-9]*"' test-results/integration/results.xml 2>/dev/null | sed 's/errors="\([0-9]*\)"/\1/' || echo "0")
            int_unit_suite_skipped=$(grep -o 'skipped="[0-9]*"' test-results/integration/results.xml 2>/dev/null | sed 's/skipped="\([0-9]*\)"/\1/' || echo "0")
            
            # Fallback: Count individual elements
            int_unit_element_total=$(grep -c '<testcase' test-results/integration/results.xml 2>/dev/null || echo "0")
            int_unit_element_failures=$(grep -c '<failure' test-results/integration/results.xml 2>/dev/null || echo "0")
            int_unit_element_errors=$(grep -c '<error' test-results/integration/results.xml 2>/dev/null || echo "0")
            int_element_skipped=$(grep -c '<skipped' test-results/integration/results.xml 2>/dev/null || echo "0")
            
            # Use testsuite attributes if available, otherwise fall back to element counts
            if [ "$int_unit_suite_total" != "0" ] && [ "$int_unit_suite_total" != "" ]; then
              int_total_tests=$int_unit_suite_total
              int_failed_tests=$int_unit_suite_failures
              int_error_tests=$int_unit_suite_errors
              int_skipped_tests=$int_unit_suite_skipped
            else
              int_total_tests=$int_unit_element_total
              int_failed_tests=$int_unit_element_failures
              int_error_tests=$int_unit_element_errors
              int_skipped_tests=$int_element_skipped
            fi
            
            # Clean up any whitespace and ensure they are valid numbers
            int_total_tests=$(echo "$int_total_tests" | tr -d '[:space:]')
            int_failed_tests=$(echo "$int_failed_tests" | tr -d '[:space:]')
            int_error_tests=$(echo "$int_error_tests" | tr -d '[:space:]')
            int_skipped_tests=$(echo "$int_skipped_tests" | tr -d '[:space:]')
            
            # Set default values if empty
            int_total_tests=${int_total_tests:-0}
            int_failed_tests=${int_failed_tests:-0}
            int_error_tests=${int_error_tests:-0}
            int_skipped_tests=${int_skipped_tests:-0}
            
            # Calculate passed integration tests
            int_passed_tests=$((int_total_tests - int_failed_tests - int_error_tests - int_skipped_tests))
            
            # Ensure we don't have negative numbers
            if [ "$int_passed_tests" -lt 0 ]; then
              int_passed_tests=0
            fi
            
            # Create visual status indicators for integration tests
            if [ "$int_failed_tests" -eq 0 ] && [ "$int_error_tests" -eq 0 ]; then
              echo "✅ **All integration tests passed!** ($int_total_tests/$int_total_tests)" >> test-summary.md
              echo "" >> test-summary.md
              echo "🎉 Great job! All integration tests are passing." >> test-summary.md
            else
              echo "❌ **Integration tests failed!** ($int_passed_tests/$int_total_tests passed, $((int_failed_tests + int_error_tests)) failed)" >> test-summary.md
              echo "" >> test-summary.md
              echo "⚠️  Some integration tests are failing. Please fix them before merging." >> test-summary.md
            fi
            
            echo "" >> test-summary.md
            echo "### 📈 Integration Test Statistics" >> test-summary.md
            echo "- **Total Tests:** $int_total_tests" >> test-summary.md
            echo "- **Passed:** $int_passed_tests" >> test-summary.md
            echo "- **Failed:** $int_failed_tests" >> test-summary.md
            if [ "$int_error_tests" -gt 0 ]; then
              echo "- **Errors:** $int_error_tests" >> test-summary.md
            fi
            if [ "$int_skipped_tests" -gt 0 ]; then
              echo "- **Skipped:** $int_skipped_tests" >> test-summary.md
            fi
            
            # Calculate integration test success rate
            if [ "$int_total_tests" -gt 0 ]; then
              int_success_rate=$(( (int_passed_tests * 100) / int_total_tests ))
              echo "- **Success Rate:** $int_success_rate%" >> test-summary.md
            else
              echo "- **Success Rate:** 0%" >> test-summary.md
            fi
            
            # Add integration test details if there are failures or errors
            if [ "$int_failed_tests" -gt 0 ] || [ "$int_error_tests" -gt 0 ]; then
              echo "" >> test-summary.md
              echo "### 🔍 Failed Integration Tests Details" >> test-summary.md
              
              if [ "$int_failed_tests" -gt 0 ]; then
                echo "#### ❌ Integration Test Failures:" >> test-summary.md
                # Extract test names and failure messages
                grep -A 10 '<testcase' test-results/integration/results.xml 2>/dev/null | while IFS= read -r line; do
                  if [[ $line == *"name="* ]]; then
                    test_name=$(echo "$line" | sed -n 's/.*name="\([^"]*\)".*/\1/p')
                    if [ ! -z "$test_name" ]; then
                      echo "- **$test_name**" >> test-summary.md
                    fi
                  elif [[ $line == *"<failure"* ]]; then
                    # Extract failure message
                    failure_msg=$(echo "$line" | sed -n 's/.*<failure[^>]*>\(.*\)<\/failure>.*/\1/p')
                    if [ ! -z "$failure_msg" ]; then
                      echo "  - Failure: \`$failure_msg\`" >> test-summary.md
                    fi
                  fi
                done
              fi
              
              echo "" >> test-summary.md
              echo "### 📋 Raw Integration Test Results" >> test-summary.md
              echo '```xml' >> test-summary.md
              
              # Pretty print the XML for better readability
              if command -v xmllint >/dev/null 2>&1; then
                # Use xmllint if available (most Linux systems)
                xmllint --format test-results/integration/results.xml >> test-summary.md
              else
                # Use Python for pretty printing if xmllint not available
                python3 -c "import xml.dom.minidom; import sys; dom = xml.dom.minidom.parse('test-results/integration/results.xml'); print(dom.toprettyxml(indent='  '))" >> test-summary.md 2>/dev/null || cat test-results/integration/results.xml >> test-summary.md
              fi
              
              echo '```' >> test-summary.md
            fi
            
          else
            echo "❌ **No integration test results found**" >> test-summary.md
            echo "" >> test-summary.md
            echo "Integration tests may have failed before generating results. Check the workflow logs for details." >> test-summary.md
          fi
          
          # Selenium Test Results Section
          echo "" >> test-summary.md
          echo "---" >> test-summary.md
          echo "" >> test-summary.md
          echo "## 🌐 Selenium Test Results" >> test-summary.md
          echo "" >> test-summary.md
          
          # Check if selenium test results exist
          if [ -f "test-results/selenium/results.xml" ]; then
            # Parse selenium test XML results
            echo "### 📊 Selenium Test Summary" >> test-summary.md
            echo "" >> test-summary.md
            
            # Extract test counts from selenium test XML
            sel_unit_suite_total=$(grep -o 'tests="[0-9]*"' test-results/selenium/results.xml 2>/dev/null | sed 's/tests="\([0-9]*\)"/\1/' || echo "0")
            sel_unit_suite_failures=$(grep -o 'failures="[0-9]*"' test-results/selenium/results.xml 2>/dev/null | sed 's/failures="\([0-9]*\)"/\1/' || echo "0")
            sel_unit_suite_errors=$(grep -o 'errors="[0-9]*"' test-results/selenium/results.xml 2>/dev/null | sed 's/errors="\([0-9]*\)"/\1/' || echo "0")
            sel_unit_suite_skipped=$(grep -o 'skipped="[0-9]*"' test-results/selenium/results.xml 2>/dev/null | sed 's/skipped="\([0-9]*\)"/\1/' || echo "0")
            
            # Fallback: Count individual elements
            sel_unit_element_total=$(grep -c '<testcase' test-results/selenium/results.xml 2>/dev/null || echo "0")
            sel_unit_element_failures=$(grep -c '<failure' test-results/selenium/results.xml 2>/dev/null || echo "0")
            sel_unit_element_errors=$(grep -c '<error' test-results/selenium/results.xml 2>/dev/null || echo "0")
            sel_element_skipped=$(grep -c '<skipped' test-results/selenium/results.xml 2>/dev/null || echo "0")
            
            # Use testsuite attributes if available, otherwise fall back to element counts
            if [ "$sel_unit_suite_total" != "0" ] && [ "$sel_unit_suite_total" != "" ]; then
              sel_total_tests=$sel_unit_suite_total
              sel_failed_tests=$sel_unit_suite_failures
              sel_error_tests=$sel_unit_suite_errors
              sel_skipped_tests=$sel_unit_suite_skipped
            else
              sel_total_tests=$sel_unit_element_total
              sel_failed_tests=$sel_unit_element_failures
              sel_error_tests=$sel_unit_element_errors
              sel_skipped_tests=$sel_element_skipped
            fi
            
            # Clean up any whitespace and ensure they are valid numbers
            sel_total_tests=$(echo "$sel_total_tests" | tr -d '[:space:]')
            sel_failed_tests=$(echo "$sel_failed_tests" | tr -d '[:space:]')
            sel_error_tests=$(echo "$sel_error_tests" | tr -d '[:space:]')
            sel_skipped_tests=$(echo "$sel_skipped_tests" | tr -d '[:space:]')
            
            # Set default values if empty
            sel_total_tests=${sel_total_tests:-0}
            sel_failed_tests=${sel_failed_tests:-0}
            sel_error_tests=${sel_error_tests:-0}
            sel_skipped_tests=${sel_skipped_tests:-0}
            
            # Calculate passed selenium tests
            sel_passed_tests=$((sel_total_tests - sel_failed_tests - sel_error_tests - sel_skipped_tests))
            
            # Ensure we don't have negative numbers
            if [ "$sel_passed_tests" -lt 0 ]; then
              sel_passed_tests=0
            fi
            
            # Create visual status indicators for selenium tests
            if [ "$sel_failed_tests" -eq 0 ] && [ "$sel_error_tests" -eq 0 ]; then
              echo "✅ **All selenium tests passed!** ($sel_total_tests/$sel_total_tests)" >> test-summary.md
              echo "" >> test-summary.md
              echo "🎉 Great job! All selenium tests are passing." >> test-summary.md
            else
              echo "❌ **Selenium tests failed!** ($sel_passed_tests/$sel_total_tests passed, $((sel_failed_tests + sel_error_tests)) failed)" >> test-summary.md
              echo "" >> test-summary.md
              echo "⚠️  Some selenium tests are failing. Please fix them before merging." >> test-summary.md
            fi
            
            echo "" >> test-summary.md
            echo "### 📈 Selenium Test Statistics" >> test-summary.md
            echo "- **Total Tests:** $sel_total_tests" >> test-summary.md
            echo "- **Passed:** $sel_passed_tests" >> test-summary.md
            echo "- **Failed:** $sel_failed_tests" >> test-summary.md
            if [ "$sel_error_tests" -gt 0 ]; then
              echo "- **Errors:** $sel_error_tests" >> test-summary.md
            fi
            if [ "$sel_skipped_tests" -gt 0 ]; then
              echo "- **Skipped:** $sel_skipped_tests" >> test-summary.md
            fi
            
            # Calculate selenium test success rate
            if [ "$sel_total_tests" -gt 0 ]; then
              sel_success_rate=$(( (sel_passed_tests * 100) / sel_total_tests ))
              echo "- **Success Rate:** $sel_success_rate%" >> test-summary.md
            else
              echo "- **Success Rate:** 0%" >> test-summary.md
            fi
            
            # Add selenium test details if there are failures or errors
            if [ "$sel_failed_tests" -gt 0 ] || [ "$sel_error_tests" -gt 0 ]; then
              echo "" >> test-summary.md
              echo "### 🔍 Failed Selenium Tests Details" >> test-summary.md
              
              if [ "$sel_failed_tests" -gt 0 ]; then
                echo "#### ❌ Selenium Test Failures:" >> test-summary.md
                # Extract test names and failure messages
                grep -A 10 '<testcase' test-results/selenium/results.xml 2>/dev/null | while IFS= read -r line; do
                  if [[ $line == *"name="* ]]; then
                    test_name=$(echo "$line" | sed -n 's/.*name="\([^"]*\)".*/\1/p')
                    if [ ! -z "$test_name" ]; then
                      echo "- **$test_name**" >> test-summary.md
                    fi
                  elif [[ $line == *"<failure"* ]]; then
                    # Extract failure message
                    failure_msg=$(echo "$line" | sed -n 's/.*<failure[^>]*>\(.*\)<\/failure>.*/\1/p')
                    if [ ! -z "$failure_msg" ]; then
                      echo "  - Failure: \`$failure_msg\`" >> test-summary.md
                    fi
                  fi
                done
              fi
              
              echo "" >> test-summary.md
              echo "### 📋 Raw Selenium Test Results" >> test-summary.md
              echo '```xml' >> test-summary.md
              
              # Pretty print the XML for better readability
              if command -v xmllint >/dev/null 2>&1; then
                # Use xmllint if available (most Linux systems)
                xmllint --format test-results/selenium/results.xml >> test-summary.md
              else
                # Use Python for pretty printing if xmllint not available
                python3 -c "import xml.dom.minidom; import sys; dom = xml.dom.minidom.parse('test-results/selenium/results.xml'); print(dom.toprettyxml(indent='  '))" >> test-summary.md 2>/dev/null || cat test-results/selenium/results.xml >> test-summary.md
              fi
              
              echo '```' >> test-summary.md
            fi
            
          else
            echo "❌ **No selenium test results found**" >> test-summary.md
            echo "" >> test-summary.md
            echo "Selenium tests may have failed before generating results. Check the workflow logs for details." >> test-summary.md
          fi
          
          # Overall Test Summary
          echo "" >> test-summary.md
          echo "---" >> test-summary.md
          echo "" >> test-summary.md
          echo "## 🎯 Overall Test Summary" >> test-summary.md
          echo "" >> test-summary.md
          
          # Calculate overall totals (handle missing test results gracefully)
          overall_total=$((total_tests + int_total_tests + sel_total_tests))
          overall_passed=$((passed_tests + int_passed_tests + sel_passed_tests))
          overall_failed=$((failed_tests + int_failed_tests + error_tests + int_error_tests + sel_failed_tests + sel_error_tests))
          
          # Ensure we don't have negative numbers
          if [ "$overall_total" -lt 0 ]; then
            overall_total=0
          fi
          if [ "$overall_passed" -lt 0 ]; then
            overall_passed=0
          fi
          if [ "$overall_failed" -lt 0 ]; then
            overall_failed=0
          fi
          
          if [ "$overall_failed" -eq 0 ]; then
            echo "🎉 **All tests passed!** ($overall_passed/$overall_total)" >> test-summary.md
          else
            echo "⚠️  **Some tests failed!** ($overall_passed/$overall_total passed, $overall_failed failed)" >> test-summary.md
          fi
          
          echo "" >> test-summary.md
          echo "- **Unit Tests:** $passed_tests/$total_tests passed" >> test-summary.md
          echo "- **Integration Tests:** $int_passed_tests/$int_total_tests passed" >> test-summary.md
          echo "- **Selenium Tests:** $sel_passed_tests/$sel_total_tests passed" >> test-summary.md
          echo "- **Total Tests:** $overall_total" >> test-summary.md
          
          # Calculate overall success rate safely
          if [ "$overall_total" -gt 0 ]; then
            overall_success_rate=$(( (overall_passed * 100) / overall_total ))
            echo "- **Overall Success Rate:** $overall_success_rate%" >> test-summary.md
          else
            echo "- **Overall Success Rate:** 0%" >> test-summary.md
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            try {
              const fs = require('fs');
              const testSummary = fs.readFileSync('test-summary.md', 'utf8');
              
              console.log('=== PR Comment Debug Info ===');
              console.log('Event name:', context.eventName);
              console.log('PR Number:', context.issue.number);
              console.log('Repository:', context.repo.owner + '/' + context.repo.repo);
              console.log('Actor:', context.actor);
              console.log('Test summary length:', testSummary.length);
              console.log('First 200 chars of summary:', testSummary.substring(0, 200));
              
              // Check if we can read the file
              if (!fs.existsSync('test-summary.md')) {
                console.log('ERROR: test-summary.md file not found!');
                return;
              }
              
              // Get PR number from different possible sources
              let prNumber = context.issue.number;
              if (!prNumber && github.event && github.event.pull_request) {
                prNumber = github.event.pull_request.number;
              }
              if (!prNumber && github.event && github.event.issue) {
                prNumber = github.event.issue.number;
              }
              
              console.log('Resolved PR Number:', prNumber);
              
              if (!prNumber) {
                console.log('ERROR: Could not determine PR number!');
                return;
              }
              
              // Create or update PR comment
              const { data: comments } = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: prNumber
              });
              
              console.log('Found', comments.length, 'existing comments');
              
              // Find existing test result comment (look for either unit or integration test results)
              const existingComment = comments.find(comment => 
                comment.user.type === 'Bot' && 
                (comment.body.includes('🧪 Unit Test Results') || comment.body.includes('🗄️ Integration Test Results') || comment.body.includes('🌐 Selenium Test Results'))
              );
              
              if (existingComment) {
                console.log('Updating existing comment:', existingComment.id);
                // Update existing comment
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: existingComment.id,
                  body: testSummary
                });
              } else {
                console.log('Creating new comment');
                // Create new comment
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: prNumber,
                  body: testSummary
                });
              }
              
              console.log('Comment operation completed successfully');
              console.log('=== End Debug Info ===');
              
            } catch (error) {
              console.log('ERROR in comment step:', error.message);
              console.log('Error stack:', error.stack);
              console.log('=== End Debug Info ===');
              throw error; // Re-throw to fail the step
            }
      - name: Upload Test Summary
        uses: actions/upload-artifact@v4
        with:
          name: test-summary
          path: test-summary.md